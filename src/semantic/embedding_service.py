"""
åµŒå…¥æœåŠ¡æ¨¡å— - ç”Ÿæˆå¤šæ¨¡æ€æ•°æ®çš„å‘é‡åµŒå…¥
"""

import asyncio
import numpy as np
import os
from typing import Dict, Any, List, Optional
from pathlib import Path
import torch
from transformers import AutoTokenizer, AutoModel
from sentence_transformers import SentenceTransformer
from PIL import Image
import cv2
from loguru import logger

from ..core.config import SemanticConfig


class EmbeddingService:
    """
    åµŒå…¥æœåŠ¡ - ä¸ºå¤šæ¨¡æ€æ•°æ®ç”Ÿæˆå‘é‡åµŒå…¥
    """
    
    def __init__(self, config: SemanticConfig):
        """
        åˆå§‹åŒ–åµŒå…¥æœåŠ¡
        
        Args:
            config: è¯­ä¹‰é…ç½®
        """
        self.config = config
        self.embedding_dimension = config.embedding_dimension
        
        # åˆå§‹åŒ–æ¨¡å‹
        self._init_models()
        
        logger.info(f"åµŒå…¥æœåŠ¡åˆå§‹åŒ–å®Œæˆï¼Œç»´åº¦: {self.embedding_dimension}")
    
    def _init_models(self):
        """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹"""
        try:
            # æ–‡æœ¬åµŒå…¥æ¨¡å‹
            model_name = self.config.embedding_model
            print(f"åˆå§‹åŒ–æ–‡æœ¬åµŒå…¥æ¨¡å‹: {model_name}")
            
            # è®¾ç½®æ¨¡å‹ç¼“å­˜è·¯å¾„ï¼Œé¿å…é‡å¤ä¸‹è½½
            cache_folder = os.path.expanduser("~/.cache/sentence_transformers")
            os.makedirs(cache_folder, exist_ok=True)
            
            # æ£€æŸ¥æœ¬åœ°æ˜¯å¦å·²æœ‰æ¨¡å‹
            if self._check_local_model_exists(model_name):
                print(f"âœ… ä½¿ç”¨æœ¬åœ°æ¨¡å‹: {model_name}")
                logger.info(f"ä½¿ç”¨æœ¬åœ°æ¨¡å‹: {model_name}")
            else:
                print(f"ğŸ“¥ ä¸‹è½½æ¨¡å‹: {model_name}")
                logger.info(f"å¼€å§‹ä¸‹è½½æ¨¡å‹: {model_name}")
            
            # åŠ è½½æ¨¡å‹ï¼ŒæŒ‡å®šç¼“å­˜ç›®å½•
            self.text_model = SentenceTransformer(
                model_name,
                cache_folder=cache_folder
            )
            
            # å›¾åƒåµŒå…¥æ¨¡å‹ï¼ˆä½¿ç”¨ CLIP æˆ–ç±»ä¼¼æ¨¡å‹ï¼‰
            # è¿™é‡Œå¯ä»¥æ ¹æ®éœ€è¦åŠ è½½ä¸“é—¨çš„å›¾åƒåµŒå…¥æ¨¡å‹
            self.image_model = None  # æš‚æ—¶ä½¿ç”¨æ–‡æœ¬æ¨¡å‹
            
            # éŸ³é¢‘åµŒå…¥æ¨¡å‹
            self.audio_model = None  # æš‚æ—¶ä½¿ç”¨æ–‡æœ¬æ¨¡å‹
            
            # è§†é¢‘åµŒå…¥æ¨¡å‹
            self.video_model = None  # æš‚æ—¶ä½¿ç”¨æ–‡æœ¬æ¨¡å‹
            
            logger.info("åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–åµŒå…¥æ¨¡å‹å¤±è´¥: {e}")
            raise
    
    def _check_local_model_exists(self, model_name: str) -> bool:
        """æ£€æŸ¥æœ¬åœ°æ˜¯å¦å·²æœ‰æ¨¡å‹"""
        try:
            import os
            # æ£€æŸ¥ HuggingFace ç¼“å­˜ç›®å½•
            cache_dir = os.path.expanduser("~/.cache/huggingface/hub")
            model_path = os.path.join(cache_dir, "models--" + model_name.replace("/", "--"))
            
            if os.path.exists(model_path):
                return True
            
            # æ£€æŸ¥ SentenceTransformers ç¼“å­˜ç›®å½•
            st_cache_dir = os.path.expanduser("~/.cache/sentence_transformers")
            st_model_path = os.path.join(st_cache_dir, model_name.replace("/", "_"))
            
            if os.path.exists(st_model_path):
                return True
            
            return False
            
        except Exception:
            return False
    
    async def generate_embedding(self, data: Dict[str, Any]) -> np.ndarray:
        """
        ç”Ÿæˆæ•°æ®åµŒå…¥
        
        Args:
            data: æ•°æ®å¯¹è±¡
        
        Returns:
            å‘é‡åµŒå…¥
        """
        try:
            data_type = data.get('type', 'text')
            data_content = data.get('data', '')
            
            if data_type == 'text':
                return await self._generate_text_embedding(data_content)
            elif data_type == 'image':
                return await self._generate_image_embedding(data_content)
            elif data_type == 'video':
                return await self._generate_video_embedding(data_content)
            elif data_type == 'audio':
                return await self._generate_audio_embedding(data_content)
            elif data_type == 'iot':
                return await self._generate_iot_embedding(data_content)
            else:
                # é»˜è®¤ä½¿ç”¨æ–‡æœ¬åµŒå…¥
                return await self._generate_text_embedding(str(data_content))
                
        except Exception as e:
            logger.error(f"ç”ŸæˆåµŒå…¥å¤±è´¥: {e}")
            # è¿”å›é›¶å‘é‡
            return np.zeros(self.embedding_dimension)
    
    async def _generate_text_embedding(self, text: str) -> np.ndarray:
        """ç”Ÿæˆæ–‡æœ¬åµŒå…¥"""
        try:
            # ä½¿ç”¨ sentence-transformers ç”ŸæˆåµŒå…¥
            embedding = self.text_model.encode(text)
            
            # ç¡®ä¿ç»´åº¦æ­£ç¡®
            if len(embedding) != self.embedding_dimension:
                # å¦‚æœç»´åº¦ä¸åŒ¹é…ï¼Œè¿›è¡Œæˆªæ–­æˆ–å¡«å……
                if len(embedding) > self.embedding_dimension:
                    embedding = embedding[:self.embedding_dimension]
                else:
                    embedding = np.pad(embedding, (0, self.embedding_dimension - len(embedding)))
            
            return embedding
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ–‡æœ¬åµŒå…¥å¤±è´¥: {e}")
            return np.zeros(self.embedding_dimension)
    
    async def _generate_image_embedding(self, image_path: str) -> np.ndarray:
        """ç”Ÿæˆå›¾åƒåµŒå…¥"""
        try:
            # è¿™é‡Œåº”è¯¥ä½¿ç”¨ä¸“é—¨çš„å›¾åƒåµŒå…¥æ¨¡å‹
            # æš‚æ—¶ä½¿ç”¨æ–‡æœ¬æè¿°ç”ŸæˆåµŒå…¥
            
            # è¯»å–å›¾åƒ
            if Path(image_path).exists():
                image = Image.open(image_path)
                
                # ç”Ÿæˆå›¾åƒæè¿°ï¼ˆè¿™é‡Œå¯ä»¥é›†æˆå›¾åƒæè¿°æ¨¡å‹ï¼‰
                description = await self._generate_image_description(image)
                
                # ä½¿ç”¨æ–‡æœ¬åµŒå…¥æ¨¡å‹
                return await self._generate_text_embedding(description)
            else:
                # å¦‚æœå›¾åƒæ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨è·¯å¾„ä½œä¸ºæè¿°
                return await self._generate_text_embedding(f"image: {image_path}")
                
        except Exception as e:
            logger.error(f"ç”Ÿæˆå›¾åƒåµŒå…¥å¤±è´¥: {e}")
            return np.zeros(self.embedding_dimension)
    
    async def _generate_video_embedding(self, video_path: str) -> np.ndarray:
        """ç”Ÿæˆè§†é¢‘åµŒå…¥"""
        try:
            # è¿™é‡Œåº”è¯¥ä½¿ç”¨ä¸“é—¨çš„è§†é¢‘åµŒå…¥æ¨¡å‹
            # æš‚æ—¶ä½¿ç”¨è§†é¢‘æè¿°ç”ŸæˆåµŒå…¥
            
            if Path(video_path).exists():
                # æå–è§†é¢‘å¸§
                frames = await self._extract_video_frames(video_path)
                
                # ç”Ÿæˆè§†é¢‘æè¿°
                description = await self._generate_video_description(frames)
                
                # ä½¿ç”¨æ–‡æœ¬åµŒå…¥æ¨¡å‹
                return await self._generate_text_embedding(description)
            else:
                return await self._generate_text_embedding(f"video: {video_path}")
                
        except Exception as e:
            logger.error(f"ç”Ÿæˆè§†é¢‘åµŒå…¥å¤±è´¥: {e}")
            return np.zeros(self.embedding_dimension)
    
    async def _generate_audio_embedding(self, audio_path: str) -> np.ndarray:
        """ç”ŸæˆéŸ³é¢‘åµŒå…¥"""
        try:
            # è¿™é‡Œåº”è¯¥ä½¿ç”¨ä¸“é—¨çš„éŸ³é¢‘åµŒå…¥æ¨¡å‹
            # æš‚æ—¶ä½¿ç”¨éŸ³é¢‘æè¿°ç”ŸæˆåµŒå…¥
            
            if Path(audio_path).exists():
                # æå–éŸ³é¢‘ç‰¹å¾
                features = await self._extract_audio_features(audio_path)
                
                # ç”ŸæˆéŸ³é¢‘æè¿°
                description = await self._generate_audio_description(features)
                
                # ä½¿ç”¨æ–‡æœ¬åµŒå…¥æ¨¡å‹
                return await self._generate_text_embedding(description)
            else:
                return await self._generate_text_embedding(f"audio: {audio_path}")
                
        except Exception as e:
            logger.error(f"ç”ŸæˆéŸ³é¢‘åµŒå…¥å¤±è´¥: {e}")
            return np.zeros(self.embedding_dimension)
    
    async def _generate_iot_embedding(self, iot_data: Dict[str, Any]) -> np.ndarray:
        """ç”Ÿæˆ IoT æ•°æ®åµŒå…¥"""
        try:
            # å°† IoT æ•°æ®è½¬æ¢ä¸ºæ–‡æœ¬æè¿°
            description = await self._generate_iot_description(iot_data)
            
            # ä½¿ç”¨æ–‡æœ¬åµŒå…¥æ¨¡å‹
            return await self._generate_text_embedding(description)
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆ IoT åµŒå…¥å¤±è´¥: {e}")
            return np.zeros(self.embedding_dimension)
    
    async def _generate_image_description(self, image: Image.Image) -> str:
        """ç”Ÿæˆå›¾åƒæè¿°"""
        # è¿™é‡Œå¯ä»¥é›†æˆå›¾åƒæè¿°æ¨¡å‹ï¼Œå¦‚ BLIP æˆ– CLIP
        # æš‚æ—¶è¿”å›åŸºæœ¬æè¿°
        return f"image with size {image.size}"
    
    async def _extract_video_frames(self, video_path: str) -> List[np.ndarray]:
        """æå–è§†é¢‘å¸§"""
        try:
            frames = []
            cap = cv2.VideoCapture(video_path)
            
            frame_count = 0
            while cap.isOpened() and frame_count < 10:  # æœ€å¤šæå– 10 å¸§
                ret, frame = cap.read()
                if not ret:
                    break
                
                frames.append(frame)
                frame_count += 1
            
            cap.release()
            return frames
            
        except Exception as e:
            logger.error(f"æå–è§†é¢‘å¸§å¤±è´¥: {e}")
            return []
    
    async def _generate_video_description(self, frames: List[np.ndarray]) -> str:
        """ç”Ÿæˆè§†é¢‘æè¿°"""
        return f"video with {len(frames)} frames"
    
    async def _extract_audio_features(self, audio_path: str) -> Dict[str, Any]:
        """æå–éŸ³é¢‘ç‰¹å¾"""
        # è¿™é‡Œå¯ä»¥é›†æˆ librosa ç­‰éŸ³é¢‘å¤„ç†åº“
        return {"duration": 0, "sample_rate": 0}
    
    async def _generate_audio_description(self, features: Dict[str, Any]) -> str:
        """ç”ŸæˆéŸ³é¢‘æè¿°"""
        return f"audio with duration {features.get('duration', 0)}s"
    
    async def _generate_iot_description(self, iot_data: Dict[str, Any]) -> str:
        """ç”Ÿæˆ IoT æ•°æ®æè¿°"""
        # å°† IoT æ•°æ®è½¬æ¢ä¸ºæ–‡æœ¬æè¿°
        description_parts = []
        
        for key, value in iot_data.items():
            if isinstance(value, (int, float)):
                description_parts.append(f"{key}: {value}")
            elif isinstance(value, str):
                description_parts.append(f"{key}: {value}")
            else:
                description_parts.append(f"{key}: {str(value)}")
        
        return "IoT data: " + ", ".join(description_parts)
    
    async def batch_generate_embeddings(self, data_list: List[Dict[str, Any]]) -> List[np.ndarray]:
        """
        æ‰¹é‡ç”ŸæˆåµŒå…¥
        
        Args:
            data_list: æ•°æ®åˆ—è¡¨
        
        Returns:
            åµŒå…¥åˆ—è¡¨
        """
        try:
            embeddings = []
            
            for data in data_list:
                embedding = await self.generate_embedding(data)
                embeddings.append(embedding)
            
            return embeddings
            
        except Exception as e:
            logger.error(f"æ‰¹é‡ç”ŸæˆåµŒå…¥å¤±è´¥: {e}")
            return [np.zeros(self.embedding_dimension)] * len(data_list)
    
    async def compute_similarity(self, embedding1: np.ndarray, embedding2: np.ndarray) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªåµŒå…¥çš„ç›¸ä¼¼åº¦
        
        Args:
            embedding1: ç¬¬ä¸€ä¸ªåµŒå…¥
            embedding2: ç¬¬äºŒä¸ªåµŒå…¥
        
        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•°
        """
        try:
            # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
            dot_product = np.dot(embedding1, embedding2)
            norm1 = np.linalg.norm(embedding1)
            norm2 = np.linalg.norm(embedding2)
            
            if norm1 == 0 or norm2 == 0:
                return 0.0
            
            similarity = dot_product / (norm1 * norm2)
            return float(similarity)
            
        except Exception as e:
            logger.error(f"è®¡ç®—ç›¸ä¼¼åº¦å¤±è´¥: {e}")
            return 0.0
    
    async def close(self):
        """å…³é—­åµŒå…¥æœåŠ¡"""
        try:
            # æ¸…ç†æ¨¡å‹èµ„æº
            if hasattr(self, 'text_model'):
                del self.text_model
            if hasattr(self, 'image_model'):
                del self.image_model
            if hasattr(self, 'audio_model'):
                del self.audio_model
            if hasattr(self, 'video_model'):
                del self.video_model
            
            logger.info("åµŒå…¥æœåŠ¡å·²å…³é—­")
        except Exception as e:
            logger.error(f"å…³é—­åµŒå…¥æœåŠ¡å¤±è´¥: {e}") 